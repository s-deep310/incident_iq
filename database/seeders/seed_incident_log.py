import sqlite3
import json
import uuid
import random
from datetime import datetime, timedelta

class AzureEnvironmentConfig:
    def __init__(self):
        self.environments = {
            "prod": {
                "subscription_id": str(uuid.uuid4()),
                "subscription_name": "Production",
                "allowed_regions": ["eastus", "westeurope", "southeastasia"],
                "resource_prefix": "prod",
                "tags": {
                    "Environment": "Production",
                    "CostCenter": "IT-Production",
                    "DataClassification": "Confidential",
                    "BusinessCriticality": "Mission-Critical"
                }
            },
            "uat": {
                "subscription_id": str(uuid.uuid4()),
                "subscription_name": "UAT",
                "allowed_regions": ["eastus2", "westus2"],
                "resource_prefix": "uat",
                "tags": {
                    "Environment": "UAT",
                    "CostCenter": "IT-Testing",
                    "DataClassification": "Internal",
                    "BusinessCriticality": "Important"
                }
            },
            "dev": {
                "subscription_id": str(uuid.uuid4()),
                "subscription_name": "Development",
                "allowed_regions": ["eastus2", "westus2"],
                "resource_prefix": "dev",
                "tags": {
                    "Environment": "Development",
                    "CostCenter": "IT-Development",
                    "DataClassification": "Internal",
                    "BusinessCriticality": "Low"
                }
            }
        }

        self.services = {
            "Microsoft.KeyVault": {
                "type": "vaults",
                "operations": [
                    "VaultGet", "KeyGet", "KeyCreate", "KeyDelete", "SecretGet",
                    "SecretSet", "SecretDelete", "CertificateCreate", "CertificateImport", "CertificateDelete"
                ],
                "resultTypes": ["Success", "Failed", "Unauthorized", "NotFound"],
                "metrics": ["ServiceApiLatency", "RequestCount", "ThrottledRequests", "Availability", "SaturationShoebox"],
                "diagnostic_categories": [
                    "AuditEvent", "AzurePolicyEvaluation", "Request", "Authentication",
                    "SecretManagement", "KeyManagement", "CertificateManagement"
                ]
                },
            "Microsoft.Sql": {
                "type": "servers/databases",
                "operations": [
                    "DatabaseConnect", "QueryExecute", "BackupComplete", "DatabaseFailover", "Login", "Logout",
                    "Deadlock", "SchemaChange", "SecurityAudit", "DataWrite", "DataRead"
                ],
                "resultTypes": ["Succeeded", "Failed", "Timeout", "Blocked"],
                "metrics": [
                    "cpu_percent", "storage_percent", "dtu_consumption_percent", "deadlock_count",
                    "failed_connections", "successful_connections", "query_duration_ms"
                ],
                "diagnostic_categories": [
                    "SQLSecurityAuditEvents", "AutomaticTuning", "QueryStoreRuntimeStatistics",
                    "QueryStoreWaitStatistics", "Error"
                ]
            },
            "Microsoft.Web": {
                "type": "sites",
                "operations": [
                    "AppServicePlanUpdate", "WebAppRestart", "SiteConfigUpdate", "AppDeployment",
                    "SlotSwap", "AppScaling"
                ],
                "resultTypes": ["Succeeded", "Failed", "InProgress", "Timeout"],
                "metrics": [
                    "Http5xx", "Http4xx", "ResponseTime", "CpuTime", "MemoryWorkingSet", "DataIn", "DataOut", "DiskQueueLength"
                ],
                "diagnostic_categories": [
                    "AppServiceHTTPLogs", "AppServiceConsoleLogs", "DetailedErrorMessages", "FailedRequestsTracing", "AppServiceEvents"
                ]
            },
            "Microsoft.Storage": {
                "type": "storageAccounts",
                "operations": [
                    "BlobGet", "BlobCreate", "BlobDelete", "ContainerDelete", "StorageRead",
                    "StorageWrite", "QueueMessageProcess", "FileOperation"
                ],
                "resultTypes": ["Success", "Failed", "Timeout", "Throttled"],
                "metrics": [
                    "Availability", "Transactions", "SuccessE2ELatency", "Ingress", "Egress", "ThrottlingError"
                ],
                "diagnostic_categories": [
                    "StorageRead", "StorageWrite", "StorageDelete", "StorageFailure", "Authentication", "Authorization"
                ]
            }
        }

        self.error_patterns = {
            "prod": {
                "critical": [
                    "High Availability Failover Initiated", "Database Deadlock Detected", "SSL Certificate Expiration Critical",
                    "Memory Resource Exhaustion", "Azure Front Door Service Disruption", "Azure SQL Database Connection Timeout",
                    "Azure VM Unresponsive", "Managed Disk Failure", "Azure Load Balancer Health Probe Failed",
                    "Azure Key Vault Access Denied"
                ],
                "high": [
                    "Elevated Error Rate Detected", "Network Connectivity Issues", "Database Performance Degradation",
                    "Azure App Service HTTP 5xx Errors", "Azure Functions Execution Timeout", "Azure Storage Throttling",
                    "API Management Gateway Latency", "Azure Cosmos DB RU Throttling", "Azure Monitor Alert Triggered"
                ]
            },
            "uat": {
                "critical": [
                    "Test Failover Simulation", "Load Test Resource Exhaustion", "Integration Test Failure",
                    "Azure DevOps Pipeline Failure", "Azure Resource Deployment Failure"
                ],
                "high": [
                    "Performance Test Threshold Breach", "API Integration Failure", "Data Sync Issues",
                    "API Rate Limit Exceeded", "Azure Service Principal Permission Denied"
                ]
            },
            "dev": {
                "critical": [
                    "Development Environment Down", "Build Pipeline Failure", "Development Database Corruption",
                    "Local Emulator Crash", "Docker Container Start Failure"
                ],
                "high": [
                    "Development API Gateway Issues", "Local Development Stack Error", "Test Data Generation Failure",
                    "Code Repository Merge Conflicts", "Unit Test Failures"
                ]
            }
        }

    def get_resource_name(self, service, env):
        prefix = self.environments[env]["resource_prefix"]
        service_short = service.split('.')[-1].lower()
        return f"{prefix}-{service_short}-{random.randint(1,999):03d}"

    def get_resource_id(self, service, env):
        subscription_id = self.environments[env]["subscription_id"]
        region = random.choice(self.environments[env]["allowed_regions"])
        rg_name = f"{self.environments[env]['resource_prefix']}-rg-{region}"
        resource_name = self.get_resource_name(service, env)
        return f"/subscriptions/{subscription_id}/resourceGroups/{rg_name}/providers/{service}/{self.services[service]['type']}/{resource_name}"

class IncidentLogGenerator:
    def __init__(self):
        self.config = AzureEnvironmentConfig()

    def generate_activity_log(self, env):
        service = random.choice(list(self.config.services.keys()))
        operation = random.choice(self.config.services[service]["operations"])
        resource_id = self.config.get_resource_id(service, env)
        status = random.choice(self.config.services[service]["resultTypes"])

        log = {
            "correlationId": str(uuid.uuid4()),
            "eventTimestamp": (datetime.utcnow() - timedelta(minutes=random.randint(0, 1440))).isoformat() + "Z",
            "category": "Administrative",
            "resourceId": resource_id,
            "operationName": {
                "value": operation,
                "localizedValue": operation
            },
            "status": {
                "value": status,
                "localizedValue": status
            },
            "subscriptionId": resource_id.split('/')[2],
            "tags": self.config.environments[env]["tags"],
            "properties": {
                "statusCode": 200 if status == "Success" else 500,
                "serviceRequestId": str(uuid.uuid4()),
                "eventCategory": "Administrative",
                "environment": env.upper()
            }
        }

        if status != "Success":
            error_type = "critical" if random.random() < 0.3 else "high"
            error_message = random.choice(self.config.error_patterns[env][error_type])
            log["properties"]["error"] = {
                "code": f"{error_type.upper()}_ERROR",
                "message": error_message
            }

        return log

    def generate_metric_log(self, env):
        service = random.choice(list(self.config.services.keys()))
        resource_id = self.config.get_resource_id(service, env)
        metric_name = random.choice(self.config.services[service]["metrics"])
        base_value = random.uniform(0, 100)
        if env == "prod":
            value = base_value * 0.7
        elif env == "uat":
            value = base_value * random.uniform(0.4, 0.9)
        else:
            value = base_value * random.uniform(0.2, 1.0)

        return {
            "time": datetime.utcnow().isoformat() + "Z",
            "resourceId": resource_id,
            "metricName": metric_name,
            "timeGrain": "PT1M",
            "value": round(value, 2),
            "tags": self.config.environments[env]["tags"],
            "properties": {
                "environment": env.upper(),
                "subscription": self.config.environments[env]["subscription_name"],
                "metric_category": "Platform",
                "unit": "Percent" if "percent" in metric_name.lower() else "Count"
            }
      }


def run(conn):
    total_records=3000
    envs = ['prod', 'uat', 'dev']
    env_distribution = {
        'prod': int(total_records * 0.5),
        'uat': int(total_records * 0.3),
        'dev': int(total_records * 0.2)
    }

    generator = IncidentLogGenerator()

    print("Generating mock data with environment distribution:")
    for env, count in env_distribution.items():
        print(f"{env.upper()}: {count} records")

    conn.execute("DELETE FROM incident_logs")

    records_created = 0
    for env in envs:
        count = env_distribution[env]
        for i in range(count):
            source_type = "ActivityLog" if random.random() < 0.7 else "MetricLog"
            log = (generator.generate_activity_log(env)
                   if source_type == "ActivityLog"
                   else generator.generate_metric_log(env))
            payload_id = str(uuid.uuid4())
            created_at = datetime.utcnow() - timedelta(minutes=random.randint(0, 1440))
            # processed_at = created_at + timedelta(minutes=random.randint(1, 60))
            
            conn.execute("""
                INSERT INTO incident_logs
                (payload_id, payload, source_type, status, created_at)
                VALUES (?, ?, ?, ?, ?)
            """, (
                payload_id,
                json.dumps(log),
                source_type,
                "new",
                created_at.isoformat(sep=' ', timespec='seconds'),
            ))

            records_created += 1
            if records_created % 500 == 0:
                print(f"Created {records_created} records...")

    conn.commit()

    # Print summary
    cursor = conn.execute("""
        SELECT 
            json_extract(payload, '$.properties.environment') as env,
            source_type,
            COUNT(*) as count
        FROM incident_logs
        GROUP BY env, source_type
    """)
    print("\nFinal Distribution:")
    for env, source_type, count in cursor.fetchall():
        print(f"Environment: {env}, Type: {source_type}, Count: {count}")

    print("\nMock data generation completed!")

def auto_run(conn):
    total_records=50
    envs = ['prod', 'uat', 'dev']
    env_distribution = {
        'prod': int(total_records * 0.5),
        'uat': int(total_records * 0.3),
        'dev': int(total_records * 0.2)
    }

    generator = IncidentLogGenerator()

    for env, count in env_distribution.items():
        print(f"{env.upper()}: {count} records")


    records_created = 0
    for env in envs:
        count = env_distribution[env]
        for i in range(count):
            source_type = "ActivityLog" if random.random() < 0.7 else "MetricLog"
            log = (generator.generate_activity_log(env)
                   if source_type == "ActivityLog"
                   else generator.generate_metric_log(env))
            payload_id = str(uuid.uuid4())
            created_at = datetime.utcnow() - timedelta(minutes=random.randint(0, 1440))
            
            conn.execute("""
                INSERT INTO incident_logs
                (payload_id, payload, source_type, status, created_at)
                VALUES (?, ?, ?, ?, ?)
            """, (
                payload_id,
                json.dumps(log),
                source_type,
                "new",
                created_at.isoformat(sep=' ', timespec='seconds'),
            ))

            records_created += 1
            if records_created % 50 == 0:
                print(f"Creating {records_created} records every 30 seconds...")

conn.commit()
